{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebe030cb",
   "metadata": {},
   "source": [
    "# Final Project\n",
    "### Automation of Merchant Media Screening for AML purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "9612384d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\maxim\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\maxim\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\maxim\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\maxim\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\maxim\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importing relevant libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from serpapi import GoogleSearch\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pickle\n",
    "import webbrowser\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "# etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "6cc3485e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading pickles (vectorizer and model)\n",
    "\n",
    "with open('vect.pkl', 'rb') as file:\n",
    "    vect = pickle.load(file)\n",
    "    \n",
    "with open('svm_model.pkl', 'rb') as file:\n",
    "    svm_model = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "be0a9f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings for script modification\n",
    "# pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "536839d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SerpApi api key\n",
    "api_key = 'cf07369d96714784e62c191673c17918e8d7274bb4037d525ee1ed50d2b795b1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "7026cdba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please, enter the legal entity name of the company you want to screen.\n",
      "Leo vegas\n"
     ]
    }
   ],
   "source": [
    "# defining the query\n",
    "print(\"Please, enter the legal entity name of the company you want to screen.\")\n",
    "screening_object = input()\n",
    "query = '\"' + screening_object + '\"'+ ' ' + 'AND (convicted OR charged OR money laundering OR tax evasion OR embezzle OR investigation OR trialed OR sentenced OR corruption OR fraud OR fine OR penalty OR terrorist)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "64d5ac8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The search query is: \"Leo vegas\" AND (convicted OR charged OR money laundering OR tax evasion OR embezzle OR investigation OR trialed OR sentenced OR corruption OR fraud OR fine OR penalty OR terrorist)\n"
     ]
    }
   ],
   "source": [
    "print('The search query is: ' + query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d110435",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "45509bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define parameters for the api request\n",
    "params = {\n",
    "  \"engine\": \"google\",\n",
    "  \"q\": query, \n",
    "    'num': '20',\n",
    "  \"api_key\": api_key\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "1fdc3dce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://serpapi.com/search\n"
     ]
    }
   ],
   "source": [
    "# api request\n",
    "search = GoogleSearch(params)\n",
    "results = search.get_dict()\n",
    "# news_results = results[\"news_results\"]\n",
    "organic_results = results[\"organic_results\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "739fcff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming results to dataframe\n",
    "results = pd.DataFrame(organic_results)\n",
    "\n",
    "# minor cleaning and dropping irrelevant columns\n",
    "# results.drop(['position', 'thumbnail'], axis=1, inplace=True)\n",
    "results = results[['title', 'link', 'snippet']]\n",
    "results['snippet'] = results['snippet'].str.replace('\\n', '')\n",
    "results['snippet'] = results['snippet'].str.lower()\n",
    "results['title'] = results['title'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "708ed290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wordnet POS function\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ, \n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "e157a90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizing and removing punctuation function\n",
    "\n",
    "# title\n",
    "def tokenizer_depunctuation_title(row):\n",
    "  tokens = word_tokenize(row['title'])\n",
    "  return [word.lower() for word in tokens if word.isalpha()]\n",
    "\n",
    "results['title_tokenized'] = results.apply(tokenizer_depunctuation_title,axis=1)\n",
    "\n",
    "# snippet\n",
    "def tokenizer_depunctuation_snippet(row):\n",
    "  tokens = word_tokenize(row['snippet'])\n",
    "  return [word.lower() for word in tokens if word.isalpha()]\n",
    "\n",
    "results['snippet_tokenized'] = results.apply(tokenizer_depunctuation_snippet,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "20b51665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatizing function\n",
    "\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "\n",
    "# title\n",
    "def lemmatizer_with_pos_title(row):\n",
    "  return [lemmatizer.lemmatize(word,get_wordnet_pos(word)) for word in row['title_tokenized']]\n",
    "\n",
    "results['title_lemmatized'] = results.apply(lemmatizer_with_pos_title,axis=1)\n",
    "\n",
    "# snippet\n",
    "def lemmatizer_with_pos_snippet(row):\n",
    "  return [lemmatizer.lemmatize(word,get_wordnet_pos(word)) for word in row['snippet_tokenized']]\n",
    "\n",
    "results['snippet_lemmatized'] = results.apply(lemmatizer_with_pos_snippet,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "0a423135",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing stopwords\n",
    "\n",
    "#title\n",
    "def remove_sw_title(row):\n",
    "  return list(set(row['title_lemmatized']).difference(stopwords.words()))\n",
    "\n",
    "results['title_no_stopwords'] = results.apply(remove_sw_title,axis=1)\n",
    "\n",
    "# snippet\n",
    "def remove_sw_snippet(row):\n",
    "  return list(set(row['snippet_lemmatized']).difference(stopwords.words()))\n",
    "\n",
    "results['snippet_no_stopwords'] = results.apply(remove_sw_snippet,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "183c3b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# restructuring function\n",
    "\n",
    "# title\n",
    "def re_structure_title(row):\n",
    "  return \" \".join(row['title_no_stopwords'])\n",
    "\n",
    "results['title_clean'] = results.apply(re_structure_title,axis=1)\n",
    "\n",
    "# snippet\n",
    "def re_structure_snippet(row):\n",
    "  return \" \".join(row['snippet_no_stopwords'])\n",
    "\n",
    "results['snippet_clean'] = results.apply(re_structure_snippet,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "46f9a5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining variable for prediction\n",
    "\n",
    "X_title = vect.transform(results['title_clean']).toarray()\n",
    " \n",
    "X_snippet = vect.transform(results['snippet_clean']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "ede9c702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# making classification\n",
    "results['label_title'] = svm_model.predict(X_title)\n",
    "results['label_snippet'] = svm_model.predict(X_snippet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "f9dd9806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating final results table\n",
    "final_results = results[['title', 'label_title', 'snippet', 'label_snippet', 'link']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "97eda436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# excluding irrelevant results\n",
    "final_results = final_results[(final_results['label_title'] == 'relevant') | (final_results['label_snippet'] == 'relevant')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "9e1a92d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_count = len(final_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "e9ec5b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A csv file containing 11 potentially relevant news has been saved on your machine! Please manually review them for relevance!\n",
      "Would you like to open the URL(s) of the results? [y/n]\n",
      "y\n",
      "The links will be opened automatically.\n"
     ]
    }
   ],
   "source": [
    "# saving file and/or concluding the script\n",
    "if results_count > 0:\n",
    "    \n",
    "    final_results.to_csv('media_' + screening_object + '.csv', index=False)\n",
    "    \n",
    "    print(\"A csv file containing \" + str(results_count) + \" potentially relevant news has been saved on your machine! Please manually review them for relevance!\")\n",
    "    \n",
    "    print(\"Would you like to open the URL(s) of the results? [y/n]\")\n",
    "    \n",
    "    while True:\n",
    "    \n",
    "        open_url = input()\n",
    "    \n",
    "        if open_url.lower() in ['y', 'yes']:\n",
    "        \n",
    "            urls = list(final_results['link'])\n",
    "            \n",
    "            print('The links will be opened automatically.')\n",
    "\n",
    "            for url in urls:\n",
    "                webbrowser.open(url)\n",
    "            break\n",
    "            \n",
    "        elif open_url.lower() in ['n', 'no']:\n",
    "        \n",
    "            print(\"The links will not be opened automatically.\")\n",
    "            \n",
    "            break\n",
    "        \n",
    "        else:\n",
    "    \n",
    "            print(\"Invalid answer. Try 'yes' or 'no'.\")\n",
    "    \n",
    "else:\n",
    "    \n",
    "     print(\"No relevant media was found. To be sure, please check manually!\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c31fba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
